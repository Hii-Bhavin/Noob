{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **BeautifulSoup** : Parse the HTML elements.\n",
    "- **pandas** : To create and modify the dataframes.\n",
    "- **requests** : To get access of a webpage.\n",
    "- **time** : To get the current time.\n",
    "- **datetime** : To modify date format.\n",
    "- **gspread** : To access google sheets through it's API.\n",
    "- **oauth2client** : To access google sheets through it's API.\n",
    "- **concurrent.futures** : To execute multiple links at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Importing all required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import gspread\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have created a google user account for accessing the Google Sheets API.\n",
    "- **creds** : Contains the credentials required to access the sheet\n",
    "- **client** : Authorises our credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Installing gspread and setup of client\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "scope = [\"https://spreadsheets.google.com/feeds\",'https://www.googleapis.com/auth/spreadsheets',\"https://www.googleapis.com/auth/drive.file\",\"https://www.googleapis.com/auth/drive\"]\n",
    "\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(\"creds2020.json\",scope)\n",
    "client = gspread.authorize(creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **sheet** : The sheet we want to access\n",
    "- **target_sheet** : The worksheet we want to edit in a particular sheet\n",
    "- **target_sheet.update_acell('A1' , 'Your_text')** : To check that data get feed in right location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Assingment of the worksheet No.1 and dry check\n",
    "sheet = client.open(\"TOI2020\")\n",
    "target_sheet = sheet.get_worksheet(0)\n",
    "target_sheet.update_acell('A41504' , 'DateX')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Structure of webpages\n",
    "### 1. Constants Defined\n",
    "- **START_TIME** : Unique code of webpage for everyday's archieve. (Here : Start of 2020's 1st Half)\n",
    "- **INDEX_START** : Index of the dataframe and google sheets\n",
    "- **LIMIT** : Unique code of webpage for everyday's archieve. (Here : End of 2020's 1st Half)\n",
    "- **BATCH** : Index for days processed till now (Helps us to trace last update, if any error occurs)\n",
    "- **NEW_CELL** : Location of cell in which we will update our data.\n",
    "- **RETRY_DELAY** : Delay in seconds before retrying\n",
    "- **MAX_RETRY_COUNT** : Maximum number of retries before giving up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Archieve of Single day\n",
    "- All article of same day are stored in a table with class = \"_rightColWrap_\"\n",
    "- We can access all these links in table through function - **def get_links(daily_url)**\n",
    "\n",
    "### 3. Individual Article\n",
    "- We Can access all these data elements through function - **def extract_data(links)**\n",
    "- Heading : Can access directly, as whole webpage has only one Heading.\n",
    "- Date : Date is stored in div with class = \"_VEOUU_\".\n",
    "- Text : Text is stored in div with class = \"*_s30J clearfix*\"\n",
    "\n",
    "## B. Working of different functions\n",
    "1. **get_links(daily_url)** :</br> To access all individual links in a single day's archieve.\n",
    "2. **extract_data(links)** :</br> To access all data elements of a single article.\n",
    "3. **update_excel(final_dataframe, new_cell)** : </br> To update the dataframe to Google sheet.\n",
    "4. **Try and Except** : </br> try and except are used to encounter error by its own. The *try* will execute the code but this throws an error then *except* will execute code for error handling (given by the user).\n",
    "5. **except requests.exceptions.RequestException as e** : </br> This will handle any connection                       error occurs (connection lost at host or at client).\n",
    "6. **except Exception as e** : </br> This will identify the broken links and skip these links. </br>  &nbsp;  &nbsp;&nbsp;1. Link is not working. </br>  &nbsp; &nbsp; 2. The link redirects to third-party links or other publisher's website. </br> &nbsp; &nbsp; 3. The webpage is temporarily or ermanently removed.\n",
    "7. **with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor** : </br> This will exceute multiple links at once to boost the speed of our task.</br> *max_workers = 5*, is the number of cores used to perform the task (Genrally the value is taken 4-10).\n",
    "8. **time.sleep(RETRY_DELAY)** : </br> This gives us a window of some seconds to reconnect to the internet by adding some time delay when the connection is lost.\n",
    "\n",
    "\n",
    "## B. Working of basic functions\n",
    "1. **response = requests.get(daily_url)** : Sending an access request to the webpage.\n",
    "2. **html_content = response.text** : Parsing the response in text format.\n",
    "3. **soup = BeautifulSoup(html_content, 'html.parser')** : Parsing HTML elements and navigating through them.\n",
    "4. **link = soup.find('ABC', class_=\"ABC\")** : Finding a div with a particular class.\n",
    "5. **links = link.find_all('a')** :  Finding links stored in the specific div class element.4\n",
    "6. **url.get('href')** : Accessing all parsed links as hyperlink to use them.\n",
    "7. **soup.find_all(['ABC'])** : Finding all ABC elements present in our target set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Main Code\n",
    "\n",
    "# Constants\n",
    "START_TIME = 43831 \n",
    "INDEX_START = 12502\n",
    "LIMIT = 44742\n",
    "BATCH = 83\n",
    "# BATCH_SIZE = 25\n",
    "NEW_CELL = 'A41504'\n",
    "RETRY_DELAY = 50  \n",
    "MAX_RETRY_COUNT = 10 \n",
    "\n",
    "# Function to retrieve links from daily URL\n",
    "def get_links(daily_url):\n",
    "    try:\n",
    "        response = requests.get(daily_url)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        link = soup.find('tr', class_=\"rightColWrap\") \n",
    "        links = link.find_all('a')\n",
    "        return [url.get('href') for url in links]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error retrieving links from daily URL: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to extract data from link\n",
    "def extract_data(links):\n",
    "    for i in range(MAX_RETRY_COUNT):\n",
    "        try:\n",
    "            response = requests.get(links)\n",
    "            response.raise_for_status()\n",
    "            html_content = response.text\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            try:\n",
    "                # Extarcting Heading\n",
    "                headings = [tag.text.strip() for tag in soup.find_all(['h1'])]\n",
    "                heading = sorted(headings)[0]\n",
    "\n",
    "                # Extracting Date\n",
    "                datex = soup.find('div', class_='VEOUU').text\n",
    "                i = 18  # i = 18, necessary for required format \n",
    "                date = \"\" # defining an empty string\n",
    "\n",
    "                while (i < len((datex))):\n",
    "                    date = date + datex[i]\n",
    "                    i += 1\n",
    "\n",
    "                # Converting Date from January 1, 2021 to 01-01-2021\n",
    "                date_str = date \n",
    "                date_obj = datetime.strptime(date_str, \" %B %d, %Y\") #this will tell variable about the location of different elements of date. %B = January\n",
    "                formatted_date_str = date_obj.strftime(\"%d-%m-%Y\") #arranging the date elements in required format\n",
    "                Date = formatted_date_str\n",
    "\n",
    "                # Extracting Text \n",
    "                text = soup.find('div', class_='_s30J clearfix').text\n",
    "                return Date, heading, text\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting data from link: {e}\")\n",
    "                return None, None, None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error extracting data from link: {e}\")\n",
    "            time.sleep(RETRY_DELAY)\n",
    "    return None, None, None\n",
    "\n",
    "# Function to update Google Sheets\n",
    "def update_excel(final_dataframe, new_cell):\n",
    "    try:\n",
    "        if new_cell == 'A1':\n",
    "            target_sheet.update(new_cell, [final_dataframe.columns.values.tolist()] + final_dataframe.fillna(-1).values.tolist())\n",
    "        else:\n",
    "            target_sheet.update(new_cell, [final_dataframe.columns.values.tolist()] + final_dataframe.fillna(-1).values.tolist())\n",
    "        print('DataFrame is updated to Excel File successfully.')\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating Excel file: {e}\")\n",
    "\n",
    "# Main loop\n",
    "my_columns = ['Date', 'Heading', 'Text']\n",
    "final_dataframe = pd.DataFrame(columns=my_columns)\n",
    "\n",
    "t = START_TIME + BATCH\n",
    "index = INDEX_START\n",
    "batch = BATCH\n",
    "run = 0\n",
    "\n",
    "while t <= LIMIT:\n",
    "    daily_url = f'https://timesofindia.indiatimes.com/2021/1/1/archivelist/year-2021,month-1,starttime-{t}.cms'\n",
    "    batch += 1\n",
    "    t += 1\n",
    "\n",
    "    links = get_links(daily_url)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(extract_data, link) for link in links]\n",
    "\n",
    "        # Add time delay after each error\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    Date, heading, text = result\n",
    "                    index += 1\n",
    "                    final_dataframe.loc[index] = [Date, heading, text]\n",
    "                    run += 1\n",
    "                    print(f\"Batch - {batch}, Entries - {index}, Run - {run}\")\n",
    "\n",
    "                    if run % 10 == 0:  # Update Excel file every 10 runs\n",
    "                        update_excel(final_dataframe, NEW_CELL)\n",
    "            except (requests.ConnectionError, requests.Timeout, requests.HTTPError) as e:\n",
    "                print(f\"Error connecting to link: {e}\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "\n",
    "print(\"COMPLETED HALF-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We are using two separate worksheet, because the google sheets get slow if overloaded and might show some error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assingment of the worksheet No.2 and dry check\n",
    "sheet = client.open(\"TOI2020\")\n",
    "target_sheet = sheet.get_worksheet(2)\n",
    "target_sheet.update_acell('A1' , 'DateX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Code\n",
    "\n",
    "# Constants\n",
    "START_TIME = 44743\n",
    "INDEX_START = 1\n",
    "LIMIT = 44196\n",
    "BATCH = 0\n",
    "# BATCH_SIZE = 25\n",
    "NEW_CELL = 'A1'\n",
    "RETRY_DELAY = 50  # Delay in seconds before retrying\n",
    "MAX_RETRY_COUNT = 10  # Maximum number of retries before giving up\n",
    "\n",
    "# Function to retrieve links from daily URL\n",
    "def get_links(daily_url):\n",
    "    try:\n",
    "        response = requests.get(daily_url)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        link = soup.find('tr', class_=\"rightColWrap\")\n",
    "        links = link.find_all('a')\n",
    "        return [url.get('href') for url in links]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error retrieving links from daily URL: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to extract data from link\n",
    "def extract_data(links):\n",
    "    for i in range(MAX_RETRY_COUNT):\n",
    "        try:\n",
    "            response = requests.get(links)\n",
    "            response.raise_for_status()\n",
    "            html_content = response.text\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            try:\n",
    "                headings = [tag.text.strip() for tag in soup.find_all(['h1'])]\n",
    "                heading = sorted(headings)[0]\n",
    "\n",
    "                datex = soup.find('div', class_='VEOUU').text\n",
    "                i = 18  # i = 18, necessary for required format \n",
    "                date = \"\" # defining an empty string\n",
    "\n",
    "                while (i < len((datex))):\n",
    "                    date = date + datex[i]\n",
    "                    i += 1\n",
    "\n",
    "                date_str = date # Converting Date from January 1, 2021 to 01-01-2021\n",
    "                date_obj = datetime.strptime(date_str, \" %B %d, %Y\") #this will tell variable about the location of different elements of date. %B = January\n",
    "                formatted_date_str = date_obj.strftime(\"%d-%m-%Y\") #arranging the date elements in required format\n",
    "                Date = formatted_date_str\n",
    "\n",
    "                text = soup.find('div', class_='_s30J clearfix').text\n",
    "                return Date, heading, text\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting data from link: {e}\")\n",
    "                return None, None, None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error extracting data from link: {e}\")\n",
    "            time.sleep(RETRY_DELAY)\n",
    "    return None, None, None\n",
    "\n",
    "# Function to update Excel file\n",
    "def update_excel(final_dataframe, new_cell):\n",
    "    try:\n",
    "        if new_cell == 'A1':\n",
    "            target_sheet.update(new_cell, [final_dataframe.columns.values.tolist()] + final_dataframe.fillna(-1).values.tolist())\n",
    "        else:\n",
    "            target_sheet.update(new_cell, [final_dataframe.columns.values.tolist()] + final_dataframe.fillna(-1).values.tolist())\n",
    "        print('DataFrame is updated to Excel File successfully.')\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating Excel file: {e}\")\n",
    "\n",
    "# Main loop\n",
    "my_columns = ['Date', 'Heading', 'Text']\n",
    "final_dataframe = pd.DataFrame(columns=my_columns)\n",
    "\n",
    "t = START_TIME + BATCH\n",
    "index = INDEX_START\n",
    "batch = BATCH\n",
    "run = 0\n",
    "\n",
    "while t <= LIMIT:\n",
    "    daily_url = f'https://timesofindia.indiatimes.com/2021/1/1/archivelist/year-2021,month-1,starttime-{t}.cms'\n",
    "    batch += 1\n",
    "    t += 1\n",
    "\n",
    "    links = get_links(daily_url)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(extract_data, link) for link in links]\n",
    "\n",
    "        # Add time delay after each error\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    Date, heading, text = result\n",
    "                    index += 1\n",
    "                    final_dataframe.loc[index] = [Date, heading, text]\n",
    "                    run += 1\n",
    "                    print(f\"Batch - {batch}, Entries - {index}, Run - {run}\")\n",
    "\n",
    "                    if run % 10 == 0:  # Update Excel file every 10 runs\n",
    "                        update_excel(final_dataframe, NEW_CELL)\n",
    "            except (requests.ConnectionError, requests.Timeout, requests.HTTPError) as e:\n",
    "                print(f\"Error connecting to link: {e}\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "\n",
    "print(\"COMPLETED HALF-2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
