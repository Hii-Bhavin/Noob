{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "#####Installing required libraries\n\n# !pip install requests\n# !pip install xlsxwriter",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "requests library provides access to websites.\nxlsxwriter library provides tools to read, write and modify the spreadsheets.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#####Rest of your code remains unchanged\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport requests\nfrom datetime import datetime\nimport time\nimport random\nimport gspread\nfrom requests.exceptions import ConnectionError",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "'Beautifulsoup' : provides tools to navigate through the html code and extracting data from it.\n'pandas' : provides tools to make and modify dataframes to store the data.\n'requests' : provides access to websites.\n'xlsxwriter' : provides tools to read, write and modify the spreadsheets.\n'datetime' : used to change the format of date to our standard format.\n'time' : used for countdown (in delay functions).\n'random' : used for generating a random number from given range (in delay functions).\n'gspread' : used to read, write and modify the google sheets (using it's API).\n'ConnectionError' : used to indentify any error occured during runtime.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#####Google sheets API setup for the notebook\nfrom oauth2client.service_account import ServiceAccountCredentials\nscope = [\"https://spreadsheets.google.com/feeds\",'https://www.googleapis.com/auth/spreadsheets',\"https://www.googleapis.com/auth/drive.file\",\"https://www.googleapis.com/auth/drive\"]\ncreds = ServiceAccountCredentials.from_json_keyfile_name(\"creds2020.json\",scope)\nclient = gspread.authorize(creds)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "'ServiceAccountCredentials' : provides access to service accounts made for using API\nscope, contains various links required for service account to access the API and bridges output to sheet\ncreds, is the json files that contains all data of service account with a gmail and a key.\nclient, act as a mediator that authorises our credentials from the creds file.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#####Setup of required worksheet and cell number \n\n#var1 = client.open(\"Your_Sheet_Name\")\n#var2 = var1.get_worksheet(i) \n#var2_var1.update_acell('cell_no.' , 'Any_text')\nsheet = client.open(\"TOI2020\")\ntarget_sheet = sheet.get_worksheet(2) #here \"i\" is index of your worksheet\ntarget_sheet.update_acell('A1' , 'DateXXX')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Indexing of worksheets\nSheet 1 = 0 , Sheet 2 = 1 , Sheet 3 = 2\nWe can also use name of the worksheet ('name')",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#####Forming Dataframe to store and represent Data\n\n#var3 = ['Column 1' , 'Column 2']\n#var4 = pd.DataFrame(columns = var3)\nmy_columns = ['Date', 'Heading', 'Text']\nfinal_dataframe1 = pd.DataFrame(columns=my_columns)\n\n\n#####Day loop\n\nNew_cell = 'A1' #var_cell = 'your_target_cell'\nx = 44927 #Here x represent the range for all article published in a year (here is for 2023)\nindex = 0 #index is the number of articles scrapped till now (will update in the terminal/output window \nBatch = 0 #Batch is the days upto which articles are scrapped\nt = x + Batch\nlimit = 45291 #limit is the range upto which article of a single year are published (here is for 2023)\n\nwhile t <= 45291:\n    #cooldown period of 10 sec, so that website doesnot get a flooded by requests and eventually aborts the connection\n    time.sleep(10)\n    #this is the master link to access any article. Every article has a unique number which is equal to {t} in the master link\n    daily_url = f'https://timesofindia.indiatimes.com/2021/1/1/archivelist/year-2021,month-1,starttime-{t}.cms'\n    Batch += 1\n    t += 1\n\n    #####Retrieving links from the daily_url    \n    response = requests.get(daily_url) #Makes a request to url for access\n    html_content = response.text #Convert all html to text\n    soup = BeautifulSoup(html_content, 'html.parser') #navigating through different html elements\n    \n    #####Locating html element with required data\n    link = soup.find('tr', class_=\"rightColWrap\") #All links for articles published in a single day are stored in a table with class \"rightColWrap\"\n    links = link.find_all('a') #Accessing all anchor tags to get links \n\n    ###### using try and except to avoid any runtime error\n    try : \n        try:\n            #checking status of url response, 200 means connection successful\n            response = requests.get(url)\n            print(\"=\" + str(response))\n        \n        except:\n            #  Retrieving data from dataset of all links of article of same day\n            local_url = \"\"\n            for url in links:\n                index += 1\n                local_url = url.get('href') #to use link text as working hyperlink, otherwise it will comes out as a simple text\n                response_link = requests.get(local_url) #Makes a request to url for access\n                html_content = response_link.text #Convert all html to text\n                soup_link = BeautifulSoup(html_content, 'html.parser') #navigating through different html elements\n\n                try:\n                    #using to avoid, broken links\n                    test = 1\n                    \n            #### Extracting Heading\n                    headings = [] #Defining an empty list\n                    var = soup_link.find('h1').text #finding all h1 present in the link (well, each link will have only one h1)\n                    Heading = var\n\n            #### Extracting Date\n                    datex = soup_link.find('div', class_='VEOUU').text  #var = var.find('div' , class_ = 'VEOUU').text /// Date is mentioned in this element\n                    i = 18  # i = 18, necessary for required format \n                    date = \"\" # defining an empty string\n                    \n                    # will print the whole date, which lies in this range only\n                    while (i < len((datex))):\n                        date = date + datex[i]\n                        i += 1\n\n                    date_str = date # Converting Date from January 1, 2021 to 01-01-2021\n                    date_obj = datetime.strptime(date_str, \" %B %d, %Y\") #this will tell variable about the location of different elements of date. %B = January\n                    formatted_date_str = date_obj.strftime(\"%d-%m-%Y\") #arranging the date elements in required format\n                    Date = formatted_date_str  # Output: 01-01-2021\n\n            #### Extracting Text\n                    # var = var.find('div' , class_ = 'class_name').text /// extraxting the text mentioned in this elements only.\n                    txt = soup_link.find('div', class_='_s30J clearfix').text\n                    Text = txt\n                except:\n            #### If broken link is detected. (this will just pass that link)\n                    test = 0\n                    index -= 1\n                    print(\"broken Link detected\")\n\n            #### Adding all scraped data in a dataframe\n                # Using test, as it will only fill the data from correct working links\n                if (test):\n                    final_dataframe1.loc[index] = [\n                        Date,\n                        Heading,\n                        Text\n                    ]\n                print(\"Batch - \" + str(Batch) + \",\" + \"Entries - \" + str(index))\n\n            #### Updating our dataframe to the google worksheet\n                if (New_cell == 'A1'):\n                    target_sheet.update(New_cell , [final_dataframe1.columns.values.tolist() ] + final_dataframe1.fillna(-1).values.tolist())\n                else:\n                    target_sheet.update(New_cell , [final_dataframe1.columns.values.tolist() ] + final_dataframe1.fillna(-1).values.tolist())\n                print('DataFrame is updated to Excel File successfully.')\n\n\n    except (requests.ConnectionError, requests.Timeout) as e:\n        #### If, pc get disconnected from the Internet, then this will save us from unnecessary errors \n        print(\"Error:\", e)\n        print(e)\n        print(\"Connection lost. Retrying in 100 seconds...\")\n        time.sleep(100)  # Add a 10-second delay before retrying\n        try : \n            try:\n                response = requests.get(url)\n                print(\"=\" + str(response))\n            \n            except:\n                #  Retrieving data from links\n                local_url = \"\"\n                for url in links:\n                    index += 1\n                    local_url = url.get('href')\n                    response_link = requests.get(local_url)\n                    html_content = response_link.text\n                    soup_link = BeautifulSoup(html_content, 'html.parser')\n\n                    try:\n                        test = 1\n                        # Extracting Heading\n                        headings = []\n                        for tag in soup_link.find_all(['h1']):\n                            headings.append(tag.text.strip())\n\n                        sorted_headings = sorted(headings)\n                        for h1 in sorted_headings:\n                            Heading = h1\n\n                        # Extracting Date\n                        datex = soup_link.find('div', class_='VEOUU').text\n                        i = 18\n                        date = \"\"\n                        while (i < len((datex))):\n                            date = date + datex[i]\n                            i += 1\n\n                        # Converting Date from January 1, 2021 to 01-01-2021\n                        date_str = date\n                        date_obj = datetime.strptime(date_str, \" %B %d, %Y\")\n                        formatted_date_str = date_obj.strftime(\"%d-%m-%Y\")\n                        Date = formatted_date_str  # Output: 01-01-2021\n\n                        # Extracting Text\n                        txt = soup_link.find('div', class_='_s30J clearfix').text\n                        Text = txt\n                    except:\n                        test = 0\n                        index -= 1\n                        print(\"broken Link detected\")\n\n                    # Adding all scraped data in a dataframe\n                    if (test):\n                        final_dataframe1.loc[index] = [\n                            Date,\n                            Heading,\n                            Text\n                        ]\n                    run += 1\n                    print(\"Batch - \" + str(Batch) + \",\" + \"Entries - \" + str(index) + \",\" + \"Run - \" + str(run))\n                    \n                    if (New_cell == 'A1'):\n                        target_sheet.update(New_cell , [final_dataframe1.columns.values.tolist() ] + final_dataframe1.fillna(-1).values.tolist())\n                    else:\n                        target_sheet.update(New_cell , [final_dataframe1.columns.values.tolist() ] + final_dataframe1.fillna(-1).values.tolist())\n                    print('DataFrame is updated to Excel File successfully.')\n\n        except (requests.ConnectionError, requests.Timeout) as e:\n            print(\"Again Error:\", e)\n            print(e)\n            print(\"Connection lost. Retrying in another 100 seconds...\")\n            time.sleep(600)  # Add a another 600-second delay before retrying\n\n            #  Retrieving data from links\n            local_url = \"\"\n            for url in links:\n                            index += 1\n                            local_url = url.get('href')\n                            response_link = requests.get(local_url)\n                            html_content = response_link.text\n                            soup_link = BeautifulSoup(html_content, 'html.parser')\n\n                            try:\n                                test = 1\n                                # Extracting Heading\n                                headings = []\n                                for tag in soup_link.find_all(['h1']):\n                                    headings.append(tag.text.strip())\n\n                                sorted_headings = sorted(headings)\n                                for h1 in sorted_headings:\n                                    Heading = h1\n\n                                # Extracting Date\n                                datex = soup_link.find('div', class_='VEOUU').text\n                                i = 18\n                                date = \"\"\n                                while (i < len((datex))):\n                                    date = date + datex[i]\n                                    i += 1\n\n                                # Converting Date from January 1, 2021 to 01-01-2021\n                                date_str = date\n                                date_obj = datetime.strptime(date_str, \" %B %d, %Y\")\n                                formatted_date_str = date_obj.strftime(\"%d-%m-%Y\")\n                                Date = formatted_date_str  # Output: 01-01-2021\n\n                                # Extracting Text\n                                txt = soup_link.find('div', class_='_s30J clearfix').text\n                                Text = txt\n                            except:\n                                test = 0\n                                index -= 1\n                                print(\"broken Link detected\")\n\n                            # Adding all scraped data in a dataframe\n                            if (test):\n                                final_dataframe1.loc[index] = [\n                                    Date,\n                                    Heading,\n                                    Text\n                                ]\n                            run += 1\n                            print(\"Batch - \" + str(Batch) + \",\" + \"Entries - \" + str(index) + \",\" + \"Run - \" + str(run))\n                            \n                            if (New_cell == 'A1'):\n                                target_sheet.update(New_cell , [final_dataframe1.columns.values.tolist() ] + final_dataframe1.fillna(-1).values.tolist())\n                            else:\n                                target_sheet.update(New_cell , [final_dataframe1.columns.values.tolist() ] + final_dataframe1.fillna(-1).values.tolist())\n                            print('DataFrame is updated to Excel File successfully.')\n\n\n\nprint('DataFrame is written to Excel File successfully.')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Possible cases of Broken Links : Article removed, Link not working, Article is from any other publisher(other website)\n\ntwo nested try-except are used to avoid error due to 'connection lost'. First, will wait to 100 seconds for pc to reconnect and it will move forward if connection is established again. Second, will wait for another 600 seconds for connection to establish.",
      "metadata": {}
    }
  ]
}